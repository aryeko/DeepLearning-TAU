{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a910040b-57eb-4027-94de-f6134b653015",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b71334b5-be58-41e6-ba21-3724a91ff866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, W, b, activation, d_activation):\n",
    "        self._weights = W # W should be matrix outDim*inDim\n",
    "        self._bias = b # W should be vector outDim*1\n",
    "        self._activation = activation\n",
    "        self._d_activation = d_activation\n",
    "        \n",
    "    def forward(self, v_prev):\n",
    "        self._v_prev = v_prev\n",
    "        self._z = np.dot(self._weights, self._v_prev) + self._bias\n",
    "        self._V = self._activation(self._z)\n",
    "        \n",
    "        return self._V\n",
    "        \n",
    "    def backward(self, err_next):\n",
    "        self.G = np.multiply(self._d_activation(self._z), err_next)\n",
    "        self._w_grad = np.dot(self.G, self._v_prev.transpose())\n",
    "        \n",
    "        self._b_grad = np.dot(self.G, np.ones([self.G.shape[1], 1]))\n",
    "        \n",
    "        self._err = np.dot(self._weights.transpose(), err_next)\n",
    "                       \n",
    "        return self._err\n",
    "        \n",
    "    def step(self, rate):\n",
    "        self._weights -= rate*self._w_grad\n",
    "        self._bias -= rate*self._b_grad\n",
    "        \n",
    "    def predict(self, X):\n",
    "        Z = np.dot(self._weights, X) + self._bias\n",
    "        V = self._activation(Z)\n",
    "        \n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bb15f3-36bb-434e-ab73-07d7959dba1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    identity = lambda x: x\n",
    "    didentity = lambda x: 1\n",
    "    \n",
    "    def __init__(self, W, b):\n",
    "        super().__init__(W, b, LinearLayer.identity, LinearLayer.didentity)\n",
    "\n",
    "        \n",
    "class RelULayer(Layer):\n",
    "    RelU = lambda x: np.maximum(x,0)\n",
    "    dRelU = lambda x: (x > 0) * 1\n",
    "    \n",
    "    def __init__(self, W, b):\n",
    "        super().__init__(W, b, RelULayer.RelU, RelULayer.dRelU)\n",
    "        \n",
    "        \n",
    "class SigmoidLayer(Layer):\n",
    "    sigmoid = lambda x: 1/(1 + np.exp(-x))\n",
    "    dsigmoid = lambda x: x * (1 - x)\n",
    "    \n",
    "    def __init__(self, W, b):\n",
    "        super().__init__(W, b, SigmoidLayer.sigmoid, SigmoidLayer.dsigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f2c10-3008-45f0-a8dd-ac79eee422e6",
   "metadata": {},
   "source": [
    "## Checking HW inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eeac58f2-c410-4e34-a05a-47cd3296f522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually running the network with HW inputs\n",
    "\n",
    "X = np.array([[1],[2],[-1]])\n",
    "Y = np.array([[0]])\n",
    "\n",
    "W_1 = np.ones([2,3])\n",
    "b_1 = np.ones([2,1])\n",
    "L1 = RelULayer(W_1, b_1)\n",
    "\n",
    "W_2 = np.ones([2,2])\n",
    "b_2 = np.ones([2,1])\n",
    "L2 = RelULayer(W_2, b_2)\n",
    "\n",
    "W_3 = np.ones([1,2])\n",
    "b_3 = np.ones([1,1])\n",
    "L3 = LinearLayer(W_3, b_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50387710-b08c-40fd-81f7-31bf0aaa62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = L1.forward(X)\n",
    "v2 = L2.forward(v1)\n",
    "v3 = L3.forward(v2)\n",
    "y_hat = v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f20f24fd-cb1c-4e46-b6d5-f4dbee91a99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 output:\n",
      " [[3.]\n",
      " [3.]]\n",
      "=========================\n",
      "L2 output:\n",
      " [[7.]\n",
      " [7.]]\n",
      "=========================\n",
      "L3 output (y_hat):\n",
      " [[15.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"L1 output:\\n\", v1)\n",
    "print(\"=========================\")\n",
    "print(\"L2 output:\\n\", v2)\n",
    "print(\"=========================\")\n",
    "print(\"L3 output (y_hat):\\n\", v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01116fe7-8544-40c0-97d5-4a120c8a0c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 2*(y_hat-Y)\n",
    "\n",
    "err3 = L3.backward(loss)\n",
    "err2 = L2.backward(err3)\n",
    "err1 = L1.backward(err2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df44f8d-32fe-43d1-b7d7-d02fb8233691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 W grad:\n",
      " [[ 60. 120. -60.]\n",
      " [ 60. 120. -60.]]\n",
      "L1 b grad:\n",
      " [[60.]\n",
      " [60.]]\n",
      "=========================\n",
      "L2 W grad:\n",
      " [[90. 90.]\n",
      " [90. 90.]]\n",
      "L2 b grad:\n",
      " [[30.]\n",
      " [30.]]\n",
      "=========================\n",
      "L3 W grad:\n",
      " [[210. 210.]]\n",
      "L3 b grad:\n",
      " [[30.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"L1 W grad:\\n\", L1._w_grad)\n",
    "print(\"L1 b grad:\\n\", L1._b_grad)\n",
    "print(\"=========================\")\n",
    "print(\"L2 W grad:\\n\", L2._w_grad)\n",
    "print(\"L2 b grad:\\n\", L2._b_grad)\n",
    "print(\"=========================\")\n",
    "print(\"L3 W grad:\\n\", L3._w_grad)\n",
    "print(\"L3 b grad:\\n\", L3._b_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f927a95-2efb-4f3a-aa8a-e832c408c19f",
   "metadata": {},
   "source": [
    "## Putting it all together in a NN object\n",
    "- Note - added Q4 requirements in the training function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae26a18-aa83-4dd8-8033-7d609c0807bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, layers):\n",
    "        self._layers = layers\n",
    "        self._iter_loss = [np.inf]\n",
    "        \n",
    "    def train(self, X, Y, loss, d_loss, max_iterations=1000, rate=1e-5, convergece_threshold=1e-10, n_batches=1, decay_rate=None):       \n",
    "        X_batches = np.array_split(X.transpose(), n_batches)\n",
    "        Y_batches = np.array_split(Y.transpose(), n_batches)\n",
    "        \n",
    "        self._initial_learning_rate = rate\n",
    "                \n",
    "        for i in range(max_iterations):                \n",
    "            if decay_rate:\n",
    "                rate = self._initial_learning_rate * np.power(decay_rate, i/n_batches)\n",
    "            for X_b, Y_b in zip(X_batches, Y_batches):\n",
    "                out = X_b.transpose()\n",
    "                for l in self._layers:\n",
    "                    out = l.forward(out)\n",
    "                                    \n",
    "                err = d_loss(out, Y_b.transpose())\n",
    "                for l in reversed(self._layers):\n",
    "                    err = l.backward(err)\n",
    "                    l.step(rate)\n",
    "        \n",
    "            mean_loss = np.mean(loss(self.predict(X), Y))\n",
    "            self._iter_loss.append(mean_loss)\n",
    "            mean_loss_diff = np.abs(self._iter_loss[-1] - self._iter_loss[-2])\n",
    "            if mean_loss_diff < convergece_threshold:\n",
    "                return True\n",
    "            \n",
    "        return False\n",
    "    \n",
    "                \n",
    "    def predict(self, X):\n",
    "        pred = X\n",
    "        for l in self._layers:\n",
    "            pred = l.predict(pred)\n",
    "            \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c221b0d6-baef-437b-898d-7faaed1fae3c",
   "metadata": {},
   "source": [
    "## Bonus - batch normalization layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97557e2a-8065-4d0a-9898-a8675e6d56a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNormalizationLayer(LinearLayer):\n",
    "    def __init__(self, gamma, b, K):\n",
    "        super().__init__(gamma, b)\n",
    "        self._K = K # Number of batches\n",
    "        self._all_batches_means = None\n",
    "        self._all_batches_vars = None\n",
    "    \n",
    "    def _update_global_mean_var(self, batch_means, batch_vars):\n",
    "        if self._all_batches_means is None:\n",
    "            self._all_batches_means = batch_means / self._K\n",
    "            self._all_batches_vars = batch_vars / self._K\n",
    "        else:\n",
    "            self._all_batches_means += batch_means / self._K\n",
    "            self._all_batches_vars += batch_vars / self._K\n",
    "            \n",
    "    def _standardize_batch(self, X):\n",
    "        self._m = X.mean(axis=1).reshape([X.shape[0],1])\n",
    "        self._v = X.var(axis=1).reshape([X.shape[0],1])\n",
    "        X_stand = np.subtract(X,self._m)/np.sqrt(self._v + 1e-8)\n",
    "        self._update_global_mean_var(self._m, self._v)\n",
    "        \n",
    "        return X_stand\n",
    "                \n",
    "    def forward(self, v_prev):\n",
    "        self._v_prev = v_prev\n",
    "        self._z = self._standardize_batch(v_prev)\n",
    "        self._V = np.multiply(self._weights, self._z) + self._bias\n",
    "        \n",
    "        return self._V\n",
    "\n",
    "\n",
    "    def backward(self, err_next):\n",
    "        \n",
    "        self.G = err_next\n",
    "        self._b_grad = np.dot(self.G, np.ones([self.G.shape[1], 1]))\n",
    "        self._w_grad = np.dot(self.G, self._z.transpose())\n",
    "        self._w_grad = np.sum(self._w_grad, axis=1).reshape(self._b_grad.shape)\n",
    "        \n",
    "        self._err = np.multiply(self._weights, err_next)\n",
    "        # rescale the error for the previous layer\n",
    "        self._err = np.multiply(self._err, self._v)\n",
    "        self._err = np.add(self._err, self._m)\n",
    "                       \n",
    "        return self._err\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_stand = np.subtract(X,self._all_batches_means)/np.sqrt(self._all_batches_vars)\n",
    "        \n",
    "        Z = np.multiply(self._weights, X_stand) + self._bias\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da9592-b7f9-49a1-9986-18dc95e3fb9d",
   "metadata": {},
   "source": [
    "## Usage examples using Q4 data - out of the scope\n",
    "- **Note - this is not the answer for Q4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9482332d-1f83-4420-8212-a408db7403d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 10000\n",
    "n_test = 1000\n",
    "n = n_train + n_test\n",
    "p = 4\n",
    "\n",
    "X = np.random.uniform(0, 1, [n, p])\n",
    "X_train, X_test = X[:n_train].transpose(), X[(n_train+1):].transpose()\n",
    "X = X.transpose()\n",
    "\n",
    "noise = np.random.normal(0, 1, n)\n",
    "Y = X[0] - 2*X[1] + 3*X[2] - 4*X[3] + noise\n",
    "Y = Y.reshape([n, 1])\n",
    "Y_train, Y_test = Y[:n_train].transpose(), Y[(n_train+1):].transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cfece49-ea19-49f8-a37e-6bdfc3d48375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hw_network(batch_norm=False, K=1):\n",
    "    W_1 = np.ones([2,4])\n",
    "    b_1 = np.ones([2,1])\n",
    "    L1 = RelULayer(W_1, b_1)\n",
    "\n",
    "    W_2 = np.ones([2,2])\n",
    "    b_2 = np.ones([2,1])\n",
    "    L2 = RelULayer(W_2, b_2)\n",
    "\n",
    "    W_3 = np.ones([1,2])\n",
    "    b_3 = np.ones([1,1])\n",
    "    L3 = LinearLayer(W_3, b_3)\n",
    "    \n",
    "    nn = NN([L1, L2, L3])\n",
    "    \n",
    "    if batch_norm:\n",
    "        bn_gamma_1 = np.ones([4,1])\n",
    "        bn_b_1 = np.ones([4,1])\n",
    "        BN1 = BatchNormalizationLayer(bn_gamma_1, bn_b_1, K)\n",
    "        \n",
    "        bn_gamma_2 = np.ones([2,1])\n",
    "        bn_b_2 = np.ones([2,1])\n",
    "        BN2 = BatchNormalizationLayer(bn_gamma_2, bn_b_2, K)\n",
    "        \n",
    "        bn_gamma_3 = np.ones([2,1])\n",
    "        bn_b_3 = np.ones([2,1])\n",
    "        BN3 = BatchNormalizationLayer(bn_gamma_3, bn_b_3, K)\n",
    "            \n",
    "        nn = NN([BN1, L1, BN2, L2, BN3, L3])\n",
    "            \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8195e50-0921-4ea2-a55c-aa7c239e49a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "rss = lambda y_hat, y: np.power((y_hat - y), 2)\n",
    "d_rss = lambda y_hat, y: 2*(y_hat - y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bcd491d-15bb-48d6-9ca4-1fa4d1253134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged? True\n",
      "Test MSE: 3.3706496873518628\n",
      "Number of iterations: 54\n",
      "Elapsed time: 0.09917958400183124\n"
     ]
    }
   ],
   "source": [
    "t1_start = perf_counter()\n",
    "\n",
    "nn1 = get_hw_network()\n",
    "nn1_converged = nn1.train(X_train, Y_train, rss, d_rss)\n",
    "nn1_mse = np.mean(rss(nn1.predict(X_test), Y_test))\n",
    "\n",
    "t1_stop = perf_counter()\n",
    "\n",
    "print(\"Converged?\", nn1_converged)\n",
    "print(\"Test MSE:\", nn1_mse)\n",
    "print(\"Number of iterations:\", len(nn1._iter_loss))\n",
    "print(\"Elapsed time:\", t1_stop-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83f11b1a-dd50-4d9c-b9d8-1bff0286bbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged? True\n",
      "Test MSE: 3.369592016213544\n",
      "Number of iterations: 305\n",
      "Elapsed time: 0.37996045899853925\n"
     ]
    }
   ],
   "source": [
    "t1_start = perf_counter()\n",
    "\n",
    "nn2 = get_hw_network()\n",
    "nn2_converged = nn2.train(X_train, Y_train, rss, d_rss, decay_rate=0.96)\n",
    "nn2_mse = np.mean(rss(nn2.predict(X_test), Y_test))\n",
    "\n",
    "\n",
    "t1_stop = perf_counter()\n",
    "\n",
    "print(\"Converged?\", nn2_converged)\n",
    "print(\"Test MSE:\", nn2_mse)\n",
    "print(\"Number of iterations:\", len(nn2._iter_loss))\n",
    "print(\"Elapsed time:\", t1_stop-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40d10502-7652-454d-b790-3214532964ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged? False\n",
      "Test MSE: 0.9715176866335166\n",
      "Number of iterations: 1001\n",
      "Elapsed time: 4.799457793997135\n"
     ]
    }
   ],
   "source": [
    "t1_start = perf_counter()\n",
    "\n",
    "nn3 = get_hw_network()\n",
    "nn3_converged = nn3.train(X_train, Y_train, rss, d_rss, n_batches=50)\n",
    "nn3_mse = np.mean(rss(nn3.predict(X_test), Y_test))\n",
    "\n",
    "\n",
    "t1_stop = perf_counter()\n",
    "\n",
    "print(\"Converged?\", nn3_converged)\n",
    "print(\"Test MSE:\", nn3_mse)\n",
    "print(\"Number of iterations:\", len(nn3._iter_loss))\n",
    "print(\"Elapsed time:\", t1_stop-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "860bf021-04f8-4d7c-bdfe-e42fc3ea7a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged? True\n",
      "Test MSE: 0.9721611706145762\n",
      "Number of iterations: 8094\n",
      "Elapsed time: 39.67634789300064\n"
     ]
    }
   ],
   "source": [
    "t1_start = perf_counter()\n",
    "\n",
    "nn3 = get_hw_network()\n",
    "nn3_converged = nn3.train(X_train, Y_train, rss, d_rss, n_batches=50, max_iterations=10000)\n",
    "nn3_mse = np.mean(rss(nn3.predict(X_test), Y_test))\n",
    "\n",
    "\n",
    "t1_stop = perf_counter()\n",
    "\n",
    "print(\"Converged?\", nn3_converged)\n",
    "print(\"Test MSE:\", nn3_mse)\n",
    "print(\"Number of iterations:\", len(nn3._iter_loss))\n",
    "print(\"Elapsed time:\", t1_stop-t1_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4650f37a-078d-47e8-95a5-6951cfce6841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged? False\n",
      "Test MSE: 3.67399584700498\n",
      "Number of iterations: 10001\n",
      "Elapsed time: 181.0727358749973\n"
     ]
    }
   ],
   "source": [
    "t1_start = perf_counter()\n",
    "\n",
    "nn4 = get_hw_network(batch_norm=True, K=50)\n",
    "nn4_converged = nn4.train(X_train, Y_train, rss, d_rss, n_batches=50, max_iterations=10000)\n",
    "nn4_mse = np.mean(rss(nn4.predict(X_test), Y_test))\n",
    "\n",
    "\n",
    "t1_stop = perf_counter()\n",
    "\n",
    "print(\"Converged?\", nn4_converged)\n",
    "print(\"Test MSE:\", nn4_mse)\n",
    "print(\"Number of iterations:\", len(nn4._iter_loss))\n",
    "print(\"Elapsed time:\", t1_stop-t1_start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
